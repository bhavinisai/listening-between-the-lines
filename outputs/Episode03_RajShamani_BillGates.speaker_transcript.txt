[00:00:00.034 - 00:00:02.718] Speaker 1: What's the biggest misunderstanding about you?
[00:00:03.078 - 00:00:09.687] Speaker 2: Whenever you hear about somebody who's got ridiculous amounts of money, their values are different than your values.
[00:00:09.767 - 00:00:12.571] Speaker 2: You should be concerned about their agenda.
[00:00:12.751 - 00:00:13.852] Speaker 2: What's your biggest fear?
[00:00:13.872 - 00:00:17.037] Speaker 2: I'll be sad as my brain gets less capable.
[00:00:17.077 - 00:00:18.258] Speaker 2: That disappoints me.
[00:00:18.399 - 00:00:20.081] Speaker 2: Tell me one behavior we all should adopt.
[00:00:20.221 - 00:00:21.943] Speaker 2: Reading a lot, being a student.
[00:00:22.103 - 00:00:24.066] Speaker 2: That's a big part of my success.
[00:00:24.206 - 00:00:26.269] Speaker 2: Richest and most powerful men in the world.
[00:00:26.770 - 00:00:30.034] Speaker 2: Bill Gates has unleashed a technological revolution.
[00:00:30.338 - 00:00:32.481] Speaker 2: that has changed our lives.
[00:00:32.501 - 00:00:36.366] Speaker 1: Bill Gates of 25 and Bill Gates of 70, any change that you feel personally?
[00:00:36.527 - 00:00:39.711] Speaker 2: In my 20s, being a maniac was the right thing.
[00:00:39.791 - 00:00:42.235] Speaker 2: My competitors would say, oh no, you work too hard.
[00:00:42.275 - 00:00:43.837] Speaker 2: And I'd say, yes, I do.
[00:00:44.097 - 00:00:50.226] Speaker 2: If you're in a race, your 20s, when you have no wife and no children, that's the time to do it.
[00:00:52.130 - 00:00:57.356] Speaker 1: If you get an opportunity to invite three Indians for dinner, who would that be?
[00:00:57.376 - 00:00:59.718] Speaker 1: There was a mathematician, Ramanujan.
[00:01:00.038 - 00:01:01.320] Speaker 1: I would have loved to have met him.
[00:01:01.440 - 00:01:05.885] Speaker 1: Why do you think India is becoming a global talent capital for the world?
[00:01:05.905 - 00:01:08.848] Speaker 2: We would hire people from India and bring them to the United States.
[00:01:09.168 - 00:01:14.914] Speaker 2: You know, both the United States and India were kind of mad at us because we were taking, you know, the smart people and moving them.
[00:01:15.055 - 00:01:18.678] Speaker 1: In India, a lot of kids also fight with their parents on the inheritance.
[00:01:19.019 - 00:01:20.901] Speaker 1: Have your kids ever spoken to you?
[00:01:21.461 - 00:01:22.002] Speaker 1: Ha ha.
[00:01:27.874 - 00:01:34.202] Speaker 1: Before we start today's episode, all I want to say is thank you to each and every one of you.
[00:01:34.283 - 00:01:44.156] Speaker 1: I'm really grateful that we were able to sit down with one of the most influential men in the world, the Bill Gates.
[00:01:45.658 - 00:01:56.192] Speaker 1: I have been this young boy who always used to hear stories about this man, about the kind of money he has made, the kind of lives he's impacting, the kind of things he's building through Microsoft.
[00:01:56.953 - 00:01:57.714] Speaker 1: From there,
[00:01:58.434 - 00:01:59.916] Speaker 1: to be sitting in front of him.
[00:02:00.617 - 00:02:01.278] Speaker 1: It was surreal.
[00:02:01.919 - 00:02:06.325] Speaker 1: When I started the conversation, you could see it on my face that I was really nervous, really scared.
[00:02:06.345 - 00:02:20.004] Speaker 1: I didn't know what to talk about, but as we went in the conversation, the podcaster in me took over and we spoke about his fears, his misunderstandings, the mission and what is he doing today.
[00:02:20.284 - 00:02:27.154] Speaker 1: Today I want you to see this episode from a lens of a 20 year old sitting with Bill Gates,
[00:02:27.554 - 00:02:30.237] Speaker 1: and figuring out what goes on in his brain.
[00:02:31.078 - 00:02:41.290] Speaker 1: This episode is truly special because I could have never thought that Bill Gates will be on our podcast this soon in our journey.
[00:02:41.791 - 00:02:50.401] Speaker 1: I always had a belief that we will be able to sit down with the smartest people around the world, but it'll happen this soon.
[00:02:52.143 - 00:02:54.706] Speaker 1: I can't believe this right now.
[00:02:55.074 - 00:02:57.397] Speaker 1: I just want you to enjoy this episode the way I did.
[00:02:57.757 - 00:03:02.122] Speaker 1: I want you to sink in that this is happening because it's not sinking in with me.
[00:03:03.263 - 00:03:06.907] Speaker 1: And I just want to tell you that there are more episodes coming.
[00:03:07.107 - 00:03:10.611] Speaker 1: So keep supporting us and hit the subscribe button right now.
[00:03:15.777 - 00:03:17.118] Speaker 1: Welcome on Figuring Out Sir.
[00:03:17.719 - 00:03:18.840] Speaker 1: I'll tell you a little story.
[00:03:19.341 - 00:03:24.306] Speaker 1: So during the pandemic, me and my sister, we were watching the Netflix documentary.
[00:03:24.386 - 00:03:37.304] Speaker 1: inside Bill's brain and halfway through it, I told my sister that, hey, you know what, like one day I'll sit with Bill Gates and I'll directly speak to him and get inside his brain.
[00:03:37.865 - 00:03:39.307] Speaker 1: And just randomly, and she was laughing.
[00:03:40.048 - 00:03:40.889] Speaker 1: So this one's for you.
[00:03:43.112 - 00:03:44.354] Speaker 1: You did it.
[00:03:44.374 - 00:03:46.117] Speaker 1: So it's an opportunity.
[00:03:46.137 - 00:03:50.142] Speaker 1: I don't know how it just came out of my mouth and it's happening today.
[00:03:50.202 - 00:03:52.786] Speaker 1: So thank you so much for doing this.
[00:03:53.282 - 00:03:54.984] Speaker 1: You've come to India quite often.
[00:03:56.205 - 00:04:01.731] Speaker 1: Tell me something that you've observed about India, which a lot of people don't know about.
[00:04:01.751 - 00:04:10.401] Speaker 2: Well, people probably, you know, because they're here all the time, you know, they probably don't recognize how much things have changed.
[00:04:10.461 - 00:04:22.514] Speaker 2: And if you go away and come back, then you see, wow, you know, the level of entrepreneurship and the, you know, the amount of innovation that's actually taking place here.
[00:04:22.946 - 00:04:25.129] Speaker 2: Uh, it's pretty fantastic.
[00:04:25.670 - 00:04:36.003] Speaker 2: And, you know, for the foundation, we've been here originally, uh, because a lot of the health challenges were here and we still care a lot about that.
[00:04:36.144 - 00:04:50.002] Speaker 2: But, uh, now a lot of our invention is being done here, whether it's, uh, well vaccines, obviously, uh, we have some incredible partnerships, but it's broadening to, you know,
[00:04:50.306 - 00:04:58.334] Speaker 2: better seeds, better diagnostics, the ways that we can use AI for health or education.
[00:04:59.215 - 00:05:06.222] Speaker 2: So the innovation ecosystem has really exploded, and that's going to be great for India.
[00:05:06.242 - 00:05:07.423] Speaker 2: It's going to be great for the world.
[00:05:07.843 - 00:05:14.890] Speaker 1: So when you meet other leaders around the world or when you meet your billionaire friends, what's the first thing you tell them about India?
[00:05:15.811 - 00:05:18.674] Speaker 1: Is it the same thing that you just told me?
[00:05:19.074 - 00:05:26.966] Speaker 2: Yeah, we've had such a great experience in our work in India.
[00:05:28.588 - 00:05:34.837] Speaker 2: I encourage people to come and tap into the great things going on.
[00:05:36.259 - 00:05:37.962] Speaker 2: Is there anything specific you tell them?
[00:05:38.162 - 00:05:48.978] Speaker 2: Well, my connection with India goes all the way back to the Microsoft days, when we at first would hire people from India and bring them to the United States.
[00:05:49.090 - 00:05:55.859] Speaker 2: And both the United States and India were kind of mad at us, because we were taking the smart people and moving them.
[00:05:55.939 - 00:06:05.451] Speaker 2: And then they came back here to India and created the Microsoft India work, which has been absolutely fantastic.
[00:06:06.112 - 00:06:14.162] Speaker 2: So up until the year 2000, I mainly knew the tech scene.
[00:06:14.402 - 00:06:16.004] Speaker 2: Bangalore, Hyderabad.
[00:06:17.427 - 00:06:22.073] Speaker 2: Seeing the country more broadly, that's in my foundation work.
[00:06:22.093 - 00:06:30.486] Speaker 2: So Bihar, UP, seeing where we could partner and help with things.
[00:06:32.068 - 00:06:34.612] Speaker 2: I still wanna take more vacation here.
[00:06:34.652 - 00:06:39.860] Speaker 2: I've done it a little bit, but there's a lot of great places in India that I haven't been to.
[00:06:40.160 - 00:06:43.986] Speaker 1: Talking about Microsoft and taking people from here to US,
[00:06:44.386 - 00:06:51.735] Speaker 1: people like Satya Nadella, Sundar Pichai, there have been incredible CEOs around the world.
[00:06:52.476 - 00:06:57.282] Speaker 1: Why do you think India is becoming a global talent capital for the world?
[00:06:57.782 - 00:07:00.906] Speaker 1: Do you think anything special here is out there?
[00:07:02.268 - 00:07:06.733] Speaker 2: Well, you know, 20% of the world's people live here in India.
[00:07:07.314 - 00:07:14.162] Speaker 2: And India's had a particular emphasis on, you know, engineering, software, you know, and turned out
[00:07:14.370 - 00:07:16.172] Speaker 2: a lot of top people.
[00:07:16.933 - 00:07:27.664] Speaker 2: And so when you get people like Sundar Satya, who are both great at engineering and management, that's a magic combination.
[00:07:27.684 - 00:07:37.755] Speaker 2: And so these companies are looking anywhere in the world to find that mix.
[00:07:37.935 - 00:07:44.242] Speaker 2: And so India's getting about its fair share of those top leadership positions.
[00:07:44.418 - 00:07:58.239] Speaker 2: And it partly comes from having great universities, not just the IITs, but starting with them, that's using the incredible talent that's here.
[00:07:58.279 - 00:08:09.916] Speaker 1: So talking about Indian talent, you do a lot of philanthropy work here, and you've met a lot of talented people here, a lot of rich people here as well.
[00:08:11.478 - 00:08:13.842] Speaker 1: Do you, and you believe that
[00:08:13.954 - 00:08:20.540] Speaker 1: all the money that rich people have, or like people who have made wealth for themselves, they should use it for charity work, they should pledge it.
[00:08:21.181 - 00:08:29.569] Speaker 1: In India, there's a, usually a lot of people believe that as parents, it's their duty to save everything for their kids, for the inheritance, right?
[00:08:29.589 - 00:08:30.750] Speaker 1: Like the kids will get everything.
[00:08:31.070 - 00:08:32.371] Speaker 1: What do you think is the right mindset?
[00:08:32.792 - 00:08:38.677] Speaker 1: Using all the money for pledging and giving it back to the society or giving it to the children?
[00:08:40.299 - 00:08:43.922] Speaker 2: Well, I think, you know, everybody gets to decide on that.
[00:08:44.002 - 00:09:02.481] Speaker 2: You know, in my case, you know, my kids got a great upbringing education, but, you know, less than 1% of the the total wealth because I decided it wouldn't be a favor to them.
[00:09:02.501 - 00:09:04.683] Speaker 2: You know, it's not a dynasty.
[00:09:05.023 - 00:09:07.326] Speaker 2: You know, I'm not asking them to run Microsoft.
[00:09:07.366 - 00:09:11.730] Speaker 2: I want to give them a chance to have their own earnings and success.
[00:09:12.098 - 00:09:18.911] Speaker 2: you know, be significant and not overshadowed by the incredible luck and good fortune I had.
[00:09:19.412 - 00:09:21.035] Speaker 2: And different families see that differently.
[00:09:21.095 - 00:09:35.602] Speaker 2: I think the people who've made fortunes from technology are less dynastic and, you know, so they'll even, you know, take their capital.
[00:09:35.714 - 00:09:37.336] Speaker 2: and give a lot of that away.
[00:09:37.376 - 00:09:41.161] Speaker 2: You can have a view of giving away your capital or just giving away your earnings.
[00:09:41.701 - 00:09:53.436] Speaker 2: And of course, I love all philanthropy, but the tech sector is probably the most aggressive about giving most of it away.
[00:09:54.337 - 00:09:55.258] Speaker 1: Not the other sectors.
[00:09:55.278 - 00:09:59.123] Speaker 1: You have not seen that in any other kind of industry where people try to give.
[00:09:59.143 - 00:10:00.865] Speaker 2: Not to the same degree.
[00:10:00.925 - 00:10:04.850] Speaker 2: It tends to be giving more of the
[00:10:05.154 - 00:10:11.986] Speaker 2: giving some portion of the profit as opposed to the actual base capital.
[00:10:12.587 - 00:10:18.377] Speaker 1: You know, in India, a lot of kids also fight with their parents on inheritance.
[00:10:18.678 - 00:10:23.807] Speaker 1: Have your kids ever spoken to you and be like, hey, why are you not giving us everything or anything?
[00:10:24.068 - 00:10:24.769] Speaker 1: Has it ever happened?
[00:10:27.033 - 00:10:29.938] Speaker 2: You know, you don't want your kids to ever be confused
[00:10:30.754 - 00:10:33.637] Speaker 2: about your support for them and your love for them.
[00:10:33.937 - 00:10:45.168] Speaker 2: And so I do think explaining early on your philosophy, you know, that you're going to treat them all equally and that you're going to give them incredible opportunities.
[00:10:46.389 - 00:10:57.319] Speaker 2: But that, you know, the highest calling for these resources is to go back to the neediest through the foundation.
[00:10:57.880 - 00:11:00.082] Speaker 2: And, you know, they've seen the
[00:11:00.162 - 00:11:01.563] Speaker 2: success of the foundation.
[00:11:01.603 - 00:11:04.987] Speaker 2: I hope they're very proud of the foundation.
[00:11:05.247 - 00:11:14.195] Speaker 2: I've seen cases where kids actually tell their parents to be more philanthropic.
[00:11:14.496 - 00:11:29.330] Speaker 2: I think the younger generation sometimes actually is pushing against this idea of the wealth just being passed down mostly.
[00:11:29.858 - 00:11:33.642] Speaker 2: So, you know, every family is a bit different.
[00:11:33.963 - 00:11:39.549] Speaker 1: I have last question on India and then we'll get to the foundation work and try to understand you as well.
[00:11:39.589 - 00:11:49.660] Speaker 1: If you get an opportunity to invite three Indians for dinner, dead or alive, who would that be?
[00:11:51.923 - 00:11:58.130] Speaker 2: Well, I get to spend time with, you know, some incredible people in Indian, you know, I get
[00:11:58.306 - 00:12:11.238] Speaker 2: time with the Prime Minister and understand his vision and how we fit into that, this, you know, 2047 thing that, you know, everything is lining up to try to achieve that.
[00:12:11.859 - 00:12:20.327] Speaker 2: You know, I get to meet, there was a great scientist, Raj Bhan, who created the Department of Biotechnology.
[00:12:20.347 - 00:12:28.274] Speaker 2: There was a mathematician, Ramanujan, who I would have loved to have met him because he was a
[00:12:28.642 - 00:12:35.251] Speaker 2: You know, almost mysterious how he was so genius at what he was able to do.
[00:12:37.394 - 00:12:40.438] Speaker 2: You know, I got to work with Rotentata.
[00:12:40.458 - 00:12:48.208] Speaker 2: I'm getting to work with all of the amazing philanthropists and a lot of the innovators here.
[00:12:48.228 - 00:12:51.112] Speaker 2: You know, I get to see small companies early on.
[00:12:51.152 - 00:12:55.858] Speaker 2: I just went through some of these companies using AI.
[00:12:55.938 - 00:13:01.467] Speaker 2: in health, some for profit, a few non-profit.
[00:13:03.570 - 00:13:15.409] Speaker 2: We had a great 25th year celebration and this great sitar artist came and that was a pretty special thing.
[00:13:16.451 - 00:13:17.573] Speaker 2: So it'd be hard for me to pick.
[00:13:17.633 - 00:13:21.559] Speaker 2: There's so many incredible talents.
[00:13:21.579 - 00:13:23.442] Speaker 2: You can't pick three.
[00:13:23.618 - 00:13:31.049] Speaker 2: I think I defend some people by not including them.
[00:13:31.430 - 00:13:33.112] Speaker 1: Fair, fair.
[00:13:33.132 - 00:13:34.595] Speaker 1: You were talking about small companies.
[00:13:34.895 - 00:13:44.910] Speaker 1: If you were to start something from zero in India today, where would you put most of your time and what the first step you'll take?
[00:13:45.050 - 00:13:52.722] Speaker 2: Well, you'd mostly start a company because you think you enjoy working on something and you think you have a world-class understanding
[00:13:52.930 - 00:13:54.552] Speaker 2: a unique contribution.
[00:13:55.532 - 00:13:56.994] Speaker 2: For me, that was software.
[00:13:57.815 - 00:14:04.001] Speaker 2: For this day and age, if you think you understand fusion, that's great.
[00:14:04.241 - 00:14:10.707] Speaker 2: If you're successful, which that's risky, you could really improve the world.
[00:14:11.368 - 00:14:22.098] Speaker 2: AI is sort of today's equivalent of what I did when I was young to see, wow, the possibilities are
[00:14:22.434 - 00:14:23.295] Speaker 2: unlimited.
[00:14:24.617 - 00:14:32.147] Speaker 2: And so, you know, I'd probably be doing one of these new AI companies if I was starting out today.
[00:14:32.628 - 00:14:40.418] Speaker 1: Do you still feel there's opportunity, a lot of opportunities in AI to build something as significant as what you have built, or it's pretty late?
[00:14:41.079 - 00:14:47.227] Speaker 2: Well, you know, the big companies are putting a lot of money into it.
[00:14:47.708 - 00:14:52.434] Speaker 2: And the number of companies globally is
[00:14:52.706 - 00:14:54.068] Speaker 2: is huge.
[00:14:54.128 - 00:14:58.673] Speaker 2: I mean, many people didn't even know these Chinese companies were doing all this work.
[00:14:58.693 - 00:15:05.121] Speaker 2: And yet some of the best results over the last six months have come from three or four of the models there.
[00:15:05.782 - 00:15:11.889] Speaker 2: So when I got going in computing, it was a very small set of people.
[00:15:12.009 - 00:15:19.218] Speaker 2: And people thought we were crazy that we set a computer on every desk and a computer in every home.
[00:15:19.298 - 00:15:33.019] Speaker 2: And that just seemed strange to people because they couldn't see the, they didn't get software and they didn't get the exponential improvement of the chips was making computing basically free.
[00:15:34.001 - 00:15:40.491] Speaker 2: So it's, I'd say it's tougher to get out in front.
[00:15:42.034 - 00:15:43.676] Speaker 2: There will be new companies.
[00:15:43.716 - 00:15:47.442] Speaker 2: I mean, Nvidia is almost as valuable as
[00:15:47.618 - 00:16:00.631] Speaker 2: Apple and Microsoft and yet they came into prominence only really in the last five or six years although they're about 15 years old.
[00:16:00.911 - 00:16:15.506] Speaker 2: So there are opportunities and there will be some new companies that use AI to achieve unbelievable success but it'll be two or three out of 10,000 and would I be able to do that again?
[00:16:15.810 - 00:16:16.571] Speaker 2: Hard to say.
[00:16:17.191 - 00:16:22.997] Speaker 1: You talked about like when you started, a lot of people thought that, oh, what are you doing?
[00:16:23.057 - 00:16:26.100] Speaker 1: This is some random thing these guys are trying to do.
[00:16:26.160 - 00:16:27.021] Speaker 1: They may be stupid.
[00:16:27.041 - 00:16:28.763] Speaker 1: They may be thinking too big, whatever.
[00:16:29.704 - 00:16:29.824] Speaker 1: Right.
[00:16:29.844 - 00:16:40.515] Speaker 1: And over the time, every time you do something big, usually people do not understand you or anyone like not just you, like whoever wants to start something new and big.
[00:16:41.716 - 00:16:43.698] Speaker 1: In today's world or as of today,
[00:16:44.066 - 00:16:46.770] Speaker 1: What's the biggest misunderstanding about you?
[00:16:48.232 - 00:16:48.773] Speaker 1: What do you think?
[00:16:49.073 - 00:16:51.076] Speaker 1: What do people misunderstand about Bill Gates?
[00:16:52.057 - 00:17:12.626] Speaker 2: Well, whenever you hear about somebody who's got some degree of power or ridiculous amounts of money, you might think they have grand schemes and there's almost a sense that their values are different.
[00:17:12.994 - 00:17:17.861] Speaker 2: than your values and that you should be concerned about their agenda.
[00:17:18.803 - 00:17:39.954] Speaker 2: Hopefully for the people who actually do know me and how much I love the foundation work and how I work with my friends or my kids, it's very different from the people who just think, oh, wow, he's one of these guys who's pushing levers.
[00:17:40.034 - 00:17:44.540] Speaker 2: has too much money and too much authority.
[00:17:45.581 - 00:17:51.148] Speaker 1: So that people misunderstand about you and tell you about misunderstanding.
[00:17:51.469 - 00:17:56.034] Speaker 1: Have you made any major mistake that people don't know about?
[00:17:56.054 - 00:17:57.857] Speaker 2: Well, some of my mistakes are very public.
[00:17:57.877 - 00:18:02.102] Speaker 2: I mean, Microsoft, you know, we had lots of products that didn't work.
[00:18:02.162 - 00:18:08.210] Speaker 2: You know, we did a phone operating system and now, you know, Android, you know, took
[00:18:08.322 - 00:18:09.163] Speaker 2: that position.
[00:18:09.203 - 00:18:16.430] Speaker 2: So I certainly messed that up in a huge way.
[00:18:17.291 - 00:18:21.276] Speaker 2: For the foundation, we often have multiple plans.
[00:18:21.316 - 00:18:24.119] Speaker 2: We wanted to have an HIV vaccine.
[00:18:25.300 - 00:18:26.521] Speaker 2: We don't have that yet.
[00:18:27.022 - 00:18:31.847] Speaker 2: We're working on a very cheap toilet, but it's still too expensive.
[00:18:32.447 - 00:18:33.749] Speaker 2: So that's taking a lot longer.
[00:18:33.769 - 00:18:35.090] Speaker 2: We don't have polio.
[00:18:35.170 - 00:18:43.663] Speaker 2: eradication done, I'm still very committed to that, but it's taking a lot longer than we thought it would.
[00:18:45.105 - 00:18:53.938] Speaker 1: Talking about foundation work, you've impacted millions of lives and you spend a lot in making people's life better.
[00:18:55.099 - 00:18:56.561] Speaker 1: What's your biggest challenge today?
[00:18:57.723 - 00:19:05.074] Speaker 1: Because somebody who's watching this might think that at your level with so much
[00:19:05.378 - 00:19:09.242] Speaker 1: power, influence, money, you can actually fix a lot of problems.
[00:19:09.783 - 00:19:11.144] Speaker 1: What do you think is your problem?
[00:19:11.165 - 00:19:14.088] Speaker 1: What challenges do you see here?
[00:19:14.108 - 00:19:17.171] Speaker 2: Well, I love the scientific challenges.
[00:19:17.251 - 00:19:21.216] Speaker 2: I mean, we still don't fully understand malnutrition.
[00:19:22.277 - 00:19:25.761] Speaker 2: We have some tools to reduce it that it's super exciting.
[00:19:26.902 - 00:19:33.810] Speaker 2: But, you know, I'm really pushing our research workers hard that, OK,
[00:19:33.922 - 00:19:37.127] Speaker 2: You know, we need to know more about that.
[00:19:38.368 - 00:19:43.676] Speaker 2: You know, I do think getting money to help the very poorest countries, a lot of which are in Africa.
[00:19:44.237 - 00:19:51.447] Speaker 2: You know, Asia, you know, many of the countries are having good economic growth.
[00:19:51.547 - 00:20:01.682] Speaker 2: And Africa has a lot of unique governance or instability or disease challenges.
[00:20:02.914 - 00:20:10.824] Speaker 2: telling people they should, you know, help out other humans, even though they're they're far away and speak a different language.
[00:20:12.446 - 00:20:14.469] Speaker 2: That's not as easy as I thought it would be.
[00:20:15.590 - 00:20:19.576] Speaker 2: You know, right now, some of the rich countries, including my country, are cutting their aid budgets.
[00:20:20.236 - 00:20:29.368] Speaker 2: And I'm I'm very disappointed that, you know, to me, that's not, you know, the golden rule of treating people like you'd like to be treated.
[00:20:30.029 - 00:20:30.770] Speaker 1: What do you think?
[00:20:31.298 - 00:20:38.748] Speaker 1: all the people who are watching this, including me, young people should do so that we could convince more people to help other people.
[00:20:39.429 - 00:20:53.709] Speaker 2: Well, when you're young, if you're here in India, you can probably travel some modest distance, an hour or two, and see people who don't have the same opportunities you do.
[00:20:53.769 - 00:20:57.714] Speaker 2: They're smart, but their school's not good, or they get
[00:20:57.922 - 00:21:02.468] Speaker 2: uh, some health problem and they're not able to access things.
[00:21:02.588 - 00:21:16.205] Speaker 2: I would say in India, you know, programs like the aspirational district program, India does, uh, talk openly about the places that are the worst off and that's, you know, that's pretty impressive.
[00:21:16.686 - 00:21:25.116] Speaker 2: Uh, you know, even issues like sanitation that governments, most governments, you know, you wouldn't catch them talking about toilets.
[00:21:25.136 - 00:21:26.418] Speaker 2: Uh, you know, this,
[00:21:26.722 - 00:21:33.329] Speaker 2: country took on a pretty aggressive plan and made a lot of progress on that.
[00:21:33.449 - 00:21:40.556] Speaker 2: So you can go out and see people and develop empathy without going too far.
[00:21:40.596 - 00:21:48.544] Speaker 1: And you've spent hours and hours and days in this country.
[00:21:49.104 - 00:21:54.770] Speaker 1: You've spent millions on vaccines and trying to take care of health care challenges.
[00:21:55.330 - 00:21:59.195] Speaker 1: What's one problem you feel that money cannot fix?
[00:22:01.158 - 00:22:25.010] Speaker 2: You know there's a lot of talk about obesity and you know when I saw the prime minister he was talking about various yoga type things that you know people would adopt those but it's been hard you know not many countries have gotten the behavior change you know maybe India can pioneer some approaches there
[00:22:25.250 - 00:22:41.555] Speaker 2: But frankly, and I'll sound like a technologist, the most promising thing is actually a class of drugs called these GLP-1 drugs that are going to go off patent and become cheap.
[00:22:41.695 - 00:22:47.003] Speaker 2: So I'm a little over-focused on a scientific solution.
[00:22:47.464 - 00:22:51.550] Speaker 2: So maybe a combination of that behavior change and the new tools.
[00:22:51.670 - 00:22:54.194] Speaker 2: But behavior change is hard.
[00:22:54.626 - 00:23:00.454] Speaker 2: We haven't succeeded in that as much as we'd like to.
[00:23:00.975 - 00:23:02.717] Speaker 2: Tell me one behavior we all should adopt.
[00:23:04.059 - 00:23:17.799] Speaker 2: Well, the behavior that's helped me is basically being a student all the time wanting to learn things and being pretty brutal with myself of do I really understand what's going on?
[00:23:18.620 - 00:23:23.026] Speaker 2: Do I understand some AI thing or
[00:23:23.138 - 00:23:24.440] Speaker 2: some disease thing.
[00:23:24.500 - 00:23:32.631] Speaker 2: And fortunately, I can meet with people who in many cases can help me understand.
[00:23:32.711 - 00:23:38.539] Speaker 2: And then knowledge, if you're careful about building your knowledge, it all kind of connects together.
[00:23:38.679 - 00:23:53.138] Speaker 2: But reading a lot, being a student, having people who can teach me, that's been not only fun for me, but also a big part of my success.
[00:23:54.210 - 00:23:59.318] Speaker 1: So all of people who are watching this actually feels pretty validated at this point.
[00:24:01.160 - 00:24:07.009] Speaker 1: Because this whole podcast is about learning from really, really incredible people like you.
[00:24:07.690 - 00:24:14.080] Speaker 1: And we have so many questions that we just keep asking to learn more, to learn more, to get inside brains.
[00:24:14.100 - 00:24:15.642] Speaker 1: So thanks for validation.
[00:24:16.904 - 00:24:18.767] Speaker 1: Perfect.
[00:24:18.787 - 00:24:23.474] Speaker 1: What's fascinating for me this time in your journey, like this time your visit to India,
[00:24:24.386 - 00:24:26.830] Speaker 1: The full Gates Foundation board is here, right?
[00:24:26.850 - 00:24:29.774] Speaker 1: And that's pretty unique and they get to experience India.
[00:24:31.277 - 00:24:32.339] Speaker 1: What do you think?
[00:24:32.359 - 00:24:36.225] Speaker 1: What are your priorities with regards to this nation?
[00:24:36.745 - 00:24:38.348] Speaker 1: Why the whole board is here?
[00:24:38.388 - 00:24:40.672] Speaker 1: What do you guys are trying to figure out?
[00:24:41.192 - 00:24:42.314] Speaker 1: What are the things that you're doing?
[00:24:43.276 - 00:24:49.966] Speaker 2: Well, as much progress as we've been part of achieving here in India, there's still a lot to do.
[00:24:49.986 - 00:24:52.370] Speaker 2: You know, the
[00:24:52.642 - 00:24:56.606] Speaker 2: The child to death rate is about a third of what it was.
[00:24:57.827 - 00:25:02.371] Speaker 2: But it's still almost three times higher than in a rich country.
[00:25:03.212 - 00:25:06.235] Speaker 2: So we should all want to close that gap.
[00:25:06.295 - 00:25:08.557] Speaker 2: That's equity.
[00:25:09.378 - 00:25:10.840] Speaker 2: I mentioned malnutrition.
[00:25:12.261 - 00:25:14.663] Speaker 2: The country is very serious about that.
[00:25:14.743 - 00:25:19.308] Speaker 2: And yet it means that your brain never develops.
[00:25:19.688 - 00:25:21.650] Speaker 2: And sadly, if
[00:25:21.890 - 00:25:40.130] Speaker 2: whatever dietary or disease things affect you during pregnancy in your first year, even if later you get a fantastic diet, your brain and your physical capabilities, they don't adjust.
[00:25:40.170 - 00:25:42.292] Speaker 2: You're sort of permanently affected.
[00:25:42.392 - 00:25:46.977] Speaker 2: And so we're working with some great scientists here.
[00:25:46.997 - 00:25:49.901] Speaker 2: We have a lot of the tools of biology have gotten a lot better.
[00:25:49.941 - 00:25:51.282] Speaker 2: So I do think
[00:25:51.394 - 00:25:56.139] Speaker 2: Um, you know, in the next decade, we'll totally get to the bottom of that.
[00:25:57.240 - 00:25:59.983] Speaker 2: Uh, and you know, I'm, I'm thrilled to do that.
[00:26:00.023 - 00:26:12.196] Speaker 2: And a lot of the scientists we partner with, you know, including some at ICMR, but you know, lots of institutions around the country, uh, they're also committed to that.
[00:26:12.937 - 00:26:18.783] Speaker 1: So you work here a lot and with the foundation, you work around the world, a lot of things.
[00:26:19.284 - 00:26:21.266] Speaker 1: And because of all of this,
[00:26:21.794 - 00:26:26.479] Speaker 1: passion and data and this intention to try to help people.
[00:26:27.400 - 00:26:34.128] Speaker 1: You've been pretty accurate in trying to understand trends way before people, like normal people.
[00:26:34.208 - 00:26:40.254] Speaker 1: Let's say you were pretty accurate in so many things, in predicting so many epidemics as well.
[00:26:40.515 - 00:26:43.117] Speaker 1: Is there something that you know which we don't?
[00:26:45.100 - 00:26:48.063] Speaker 1: Like how do you spot these things way faster?
[00:26:48.563 - 00:26:51.026] Speaker 2: Well, the biggest change agent
[00:26:51.426 - 00:27:01.836] Speaker 2: in my lifetime has been the miracle of digital, now moving into the AI phase of that digital revolution.
[00:27:01.876 - 00:27:20.754] Speaker 2: So the fact that as a young person, I was programming at age 13 and by 18 I had my thousands of hours of really strong feedback about getting better and being pushed
[00:27:21.250 - 00:27:32.010] Speaker 2: And so it's a really lucky thing to have such a familiarity with the thing that's going to change the world.
[00:27:32.110 - 00:27:44.754] Speaker 2: And so I wrote a book called The Road Ahead a long time ago that talked about the internet and digital money and video conferencing.
[00:27:45.538 - 00:27:51.984] Speaker 2: And then when I moved into the foundation work, the health work, people in that community understand pandemics.
[00:27:52.104 - 00:27:56.188] Speaker 2: And so my saying, hey, there's a big risk.
[00:27:56.468 - 00:28:02.875] Speaker 2: And the thing that's going to kill 10 million additional people is likely to be a pandemic.
[00:28:04.656 - 00:28:08.160] Speaker 2: That's commonplace knowledge if you're in the global health community.
[00:28:08.180 - 00:28:09.581] Speaker 2: It's a very small community.
[00:28:10.722 - 00:28:14.866] Speaker 2: But I was just somebody who was listened to.
[00:28:15.234 - 00:28:31.595] Speaker 2: uh, stating, uh, this, and sadly, you know, most of the people who listened to that prediction, uh, listened to it after it came true, then, you know, what my goal was, was people to hear that and actually, uh, stop it from happening.
[00:28:31.895 - 00:28:33.577] Speaker 1: That's usually the case, right?
[00:28:33.617 - 00:28:42.869] Speaker 1: Like people listen to pieces, golden advice pieces and pieces of nuggets way after the time has been passed, right?
[00:28:42.889 - 00:28:43.890] Speaker 1: Because of this,
[00:28:44.258 - 00:28:45.800] Speaker 1: Do you fear anything today?
[00:28:46.321 - 00:28:52.369] Speaker 2: I definitely hope that we shape AI in a positive way.
[00:28:52.429 - 00:29:05.167] Speaker 2: It's such a big impact on being smarter than humans that it will change our world a lot.
[00:29:06.549 - 00:29:08.932] Speaker 2: And it's definitely new territory.
[00:29:08.952 - 00:29:10.534] Speaker 2: So I have a list of about five things.
[00:29:10.775 - 00:29:13.298] Speaker 2: AI, shaping AI properly.
[00:29:13.602 - 00:29:22.178] Speaker 2: is at the top of that list, but avoiding the next pandemic, avoiding nuclear war, bioterrorism, climate change.
[00:29:22.379 - 00:29:36.946] Speaker 2: It's only about five or six things that we need to minimize the chance of and use our additional wealth and insights against those things.
[00:29:37.282 - 00:29:38.603] Speaker 2: Do you have any personal fear?
[00:29:38.824 - 00:29:39.724] Speaker 2: What's your biggest fear?
[00:29:39.845 - 00:29:44.329] Speaker 2: It's not like I'm afraid of heights or planes or fire or anything like that.
[00:29:44.710 - 00:30:01.186] Speaker 2: I hope I'll be sad as my brain gets less capable, which as I turned 70 this year, I'd be lucky to have 20 years of being able to learn.
[00:30:02.347 - 00:30:05.010] Speaker 2: Maybe I'll get lucky and get a little bit more.
[00:30:05.186 - 00:30:23.183] Speaker 2: You know, that disappoints me because I've had such an amazing time learning things and I used to think of old people as not contributing all that much and now I've had to change my mind about how important old people are.
[00:30:24.525 - 00:30:25.966] Speaker 2: Do you feel difference?
[00:30:26.526 - 00:30:27.127] Speaker 2: I don't.
[00:30:27.447 - 00:30:28.368] Speaker 2: I don't at all.
[00:30:29.389 - 00:30:34.354] Speaker 2: You know, probably if I took an IQ test, I would do a little bit worse.
[00:30:34.434 - 00:30:35.476] Speaker 2: than when I was 25.
[00:30:35.996 - 00:30:46.812] Speaker 2: But I've accumulated enough knowledge, so wisdom can compensate a little bit for a slight reduction in intelligence.
[00:30:46.873 - 00:30:58.890] Speaker 2: And I do think it's like a muscle that if you're pushing yourself to think and learn that you stay, it really helps your capabilities a lot.
[00:31:01.234 - 00:31:04.018] Speaker 2: But yeah, I have a fear that
[00:31:04.098 - 00:31:08.063] Speaker 2: eventually I'll lose, you know, I won't want to pick up a 500 page book.
[00:31:08.083 - 00:31:10.407] Speaker 2: I'll look at it and go, are you kidding?
[00:31:10.467 - 00:31:11.288] Speaker 2: I'm done with that.
[00:31:13.010 - 00:31:15.894] Speaker 1: So with age, you have not felt any changes in your brain?
[00:31:16.795 - 00:31:17.476] Speaker 1: Not really.
[00:31:17.516 - 00:31:20.320] Speaker 2: Wow.
[00:31:20.600 - 00:31:21.682] Speaker 2: No, I don't think so.
[00:31:23.124 - 00:31:33.618] Speaker 2: You know, whenever you can't remember something, you're like, oh no, now, but, you know, so maybe a tiny bit of that, but then, you know, it's just,
[00:31:33.698 - 00:31:37.242] Speaker 2: a little bit because you're looking for it.
[00:31:37.563 - 00:31:42.348] Speaker 2: When I was in my 20s, if I can remember something, I was like, so what?
[00:31:43.950 - 00:31:45.552] Speaker 1: Is there any change you feel?
[00:31:45.933 - 00:31:52.441] Speaker 1: Because Bill Gates of 25 and Bill Gates of 70, is there any change that you feel personally?
[00:31:52.641 - 00:32:03.634] Speaker 2: Well, in my 20s, I chose to focus on one topic from age 20 to about 31 or 2.
[00:32:04.610 - 00:32:17.507] Speaker 2: I told myself, hey, I love biology and math and all these things, but I want to be the person who's advancing software faster than anyone else.
[00:32:17.588 - 00:32:30.785] Speaker 2: And so I really did narrow my focus and I didn't, you know, take much time off and I could stay in the office, you know, 72 hours and then crash.
[00:32:31.526 - 00:32:34.290] Speaker 2: And so my adrenaline,
[00:32:34.658 - 00:32:38.261] Speaker 2: and was really unique.
[00:32:38.822 - 00:32:44.848] Speaker 2: Now, my understanding of how to manage people other than myself wasn't that good.
[00:32:45.949 - 00:32:50.453] Speaker 2: I look back and I've learned a lot since then.
[00:32:50.493 - 00:33:03.506] Speaker 2: But just my stamina and focus for that period of my life, being kind of a maniac was the right thing.
[00:33:03.746 - 00:33:06.470] Speaker 2: You know, my competitors would say, oh, no, you work too hard.
[00:33:06.510 - 00:33:08.052] Speaker 2: And I'd say, yes, I do.
[00:33:09.254 - 00:33:11.977] Speaker 1: Do you think that's a great advice for every young person who's watching?
[00:33:12.278 - 00:33:13.559] Speaker 1: Well, maniac in your 20s.
[00:33:13.620 - 00:33:15.923] Speaker 2: It's not it's not for everyone.
[00:33:17.345 - 00:33:22.191] Speaker 2: But if you're if you're in a race and.
[00:33:24.114 - 00:33:30.703] Speaker 2: You know, a little bit moving a little bit faster can make a difference.
[00:33:32.045 - 00:33:32.946] Speaker 2: Then, yes, you're
[00:33:33.026 - 00:33:39.274] Speaker 2: your twenties when you have no wife and no children, that's the time to do it.
[00:33:40.475 - 00:33:44.260] Speaker 1: That's the time to be a little maniac about certain things, right?
[00:33:44.280 - 00:33:53.652] Speaker 1: You also just mentioned that you love learning, which obviously the world knows, but during this conversation also, you just talked about it a little bit again and again.
[00:33:54.233 - 00:33:56.696] Speaker 1: What are you currently learning or what do you want to learn now?
[00:33:57.056 - 00:34:01.682] Speaker 2: Well, there's a lot going on.
[00:34:02.850 - 00:34:15.425] Speaker 2: You know, AI just staying on top of that is, you know, I, I very much enjoy that and I get to sit and talk with the top people at OpenAI and I get to play around with things.
[00:34:15.926 - 00:34:20.992] Speaker 2: But then when you think, okay, now about what about AI applied to mental health care?
[00:34:21.733 - 00:34:32.626] Speaker 2: Uh, isn't that one of the most exciting things because, you know, we can never have enough therapists, uh, and even people who aren't, uh,
[00:34:33.122 - 00:34:42.473] Speaker 2: you know, suffering massively, you know, maybe we could help even people with mild symptoms.
[00:34:42.494 - 00:34:50.904] Speaker 2: So, you know, what does this AI companion look like and how can that help us?
[00:34:52.125 - 00:34:58.593] Speaker 2: My friend Reid Hoffman just wrote a book called Super Agency that he's got a really good chapter about this.
[00:34:58.693 - 00:35:02.418] Speaker 2: So I'm pushing myself to try and understand
[00:35:02.818 - 00:35:10.007] Speaker 2: What can we do there and meet the people who are pushing the boundaries?
[00:35:10.647 - 00:35:12.409] Speaker 2: Because I see such potential.
[00:35:13.090 - 00:35:19.678] Speaker 1: So what would you advise young people to start learning today and from where?
[00:35:19.698 - 00:35:20.279] Speaker 1: AI itself?
[00:35:20.599 - 00:35:26.346] Speaker 2: Well, if you have a mathematical mind, not everyone should learn AI.
[00:35:27.327 - 00:35:29.810] Speaker 2: These tools are going to be available.
[00:35:30.242 - 00:35:59.730] Speaker 2: To everyone and so you ought to be a user of AI, but the underlying Stuff, you know, that's a pretty narrow set of people who will have an opportunity to You know push on a new training method And even people who grew up with software some of them don't really get this because it's it's a bit more mathematical than it is just a a programming type thing so
[00:35:59.906 - 00:36:13.659] Speaker 2: It's as a user that, okay, if you like doing creative work, yes, AI is gonna change the world and AI hopefully can help you do things faster and better.
[00:36:14.160 - 00:36:20.346] Speaker 2: So my prescription would be yes to use it for the areas that you're excited about.
[00:36:21.126 - 00:36:27.633] Speaker 1: Any area that you're excited about, learn about that and try to make a bridge between AI and that industry.
[00:36:28.093 - 00:36:28.914] Speaker 2: Right.
[00:36:29.634 - 00:36:33.566] Speaker 2: The internet has so much educational material on it.
[00:36:34.108 - 00:36:43.538] Speaker 2: So between all that material and then an AI that can help take long documents and you can have a dialogue with it.
[00:36:43.618 - 00:36:49.565] Speaker 2: You know, you can do it in text or you can, you know, now do these generate podcasts about things.
[00:36:49.685 - 00:36:52.689] Speaker 2: It's a great time to be a learner, right?
[00:36:52.709 - 00:36:58.636] Speaker 2: You know, when I was young, I had to go to the library and, you know, read the encyclopedia alphabetically.
[00:36:59.277 - 00:37:00.318] Speaker 2: No multimedia.
[00:37:01.640 - 00:37:03.923] Speaker 2: This is a paradise.
[00:37:04.623 - 00:37:04.944] Speaker 2: Yeah.
[00:37:04.964 - 00:37:05.925] Speaker 2: Thanks to you guys.
[00:37:06.205 - 00:37:08.408] Speaker 1: The digital world.
[00:37:08.448 - 00:37:10.170] Speaker 1: You made it easy for us.
[00:37:10.751 - 00:37:13.394] Speaker 1: We don't know the world, what it looked like.
[00:37:13.506 - 00:37:15.509] Speaker 1: to go to library and read about something.
[00:37:15.830 - 00:37:20.457] Speaker 1: We get fidgety if we don't get an answer in three seconds.
[00:37:20.697 - 00:37:21.939] Speaker 1: Last two questions for you.
[00:37:22.680 - 00:37:30.372] Speaker 1: One is, in your philanthropy work with the foundation, how is India contributing to your global strategies?
[00:37:31.033 - 00:37:36.541] Speaker 2: Well, more and more of the innovation is being done in India.
[00:37:37.743 - 00:37:42.610] Speaker 2: India's got a depth of talent and a desire for
[00:37:43.010 - 00:37:44.212] Speaker 2: frugal solutions.
[00:37:45.033 - 00:37:49.619] Speaker 2: And so even though the rich world, US, Europe, you know, we have a lot of talent.
[00:37:49.659 - 00:37:58.171] Speaker 2: For some basic science thing like immunology, a lot of the new insights will keep coming from those institutions.
[00:37:58.331 - 00:38:12.530] Speaker 2: But when it comes to actually putting the pieces together, including something like AI for healthcare, you know, I was just meeting with a number of companies working on various pieces of that in India.
[00:38:12.802 - 00:38:18.388] Speaker 2: So the, you know, we need better seeds and we need better weather advice for farmers.
[00:38:19.129 - 00:38:39.730] Speaker 2: And so although we're going to continue to do a lot of implementation here in India, help, you know, get things rolled out, more and more of our sort of product research work will be done here, both because it benefits India, but, you know, we're also very good if, if
[00:38:39.906 - 00:38:42.911] Speaker 2: it's saving Indian children, really working.
[00:38:42.971 - 00:38:48.941] Speaker 2: Well, you know, our foundation has a lot of presence in Africa.
[00:38:49.422 - 00:39:04.146] Speaker 2: So even take, you know, the classic example where India was totally the leader on this digital public infrastructure, the digital money and identity, ADHAR thing, you know, funding the Africans to come here and learn and
[00:39:04.226 - 00:39:08.973] Speaker 2: for their budgets and helping to create the open source software that makes it easy for them.
[00:39:09.653 - 00:39:16.162] Speaker 2: That's now a huge agenda item, which is kind of a South-South thing with a little bit of facilitation from us.
[00:39:17.424 - 00:39:17.825] Speaker 2: Got it.
[00:39:18.486 - 00:39:20.108] Speaker 1: And here's the last question.
[00:39:22.411 - 00:39:31.303] Speaker 1: If the world had to write one sentence next to the name Bill Gates, what would you want them to write?
[00:39:31.704 - 00:39:33.426] Speaker 2: You know, I don't do my work
[00:39:33.698 - 00:39:36.141] Speaker 2: based on some epitaph.
[00:39:37.082 - 00:39:46.994] Speaker 2: Ideally, they'd say that, wow, there were these diseases around polio and malaria and malnutrition.
[00:39:47.254 - 00:40:03.314] Speaker 2: Now we don't have to think about that, partly because he championed putting more great thinking and resources into ending those problems.
[00:40:03.522 - 00:40:07.726] Speaker 2: You know, I hope people look at the word polio and go, what was that?
[00:40:08.307 - 00:40:13.713] Speaker 2: Uh, you know, when you read Dickens novels and they talk about somebody had consumption, it's like, what is that?
[00:40:13.733 - 00:40:15.214] Speaker 2: Well, that's actually TB.
[00:40:15.294 - 00:40:20.420] Speaker 2: So we still not in the UK, we don't have much, but we, we, we still do.
[00:40:20.480 - 00:40:22.862] Speaker 2: They just, uh, change the word.
[00:40:22.962 - 00:40:30.450] Speaker 2: So, you know, hopefully, uh, some problems that, that we can actually finish, uh, and,
[00:40:30.658 - 00:40:31.519] Speaker 2: and then move on.
[00:40:31.880 - 00:40:33.682] Speaker 2: Which problem do you think will be finishing?
[00:40:33.723 - 00:40:38.550] Speaker 2: Well, polio, you know, I expect even in five years.
[00:40:39.291 - 00:40:45.280] Speaker 2: And that'll give us the credibility to go after things like malaria and measles.
[00:40:46.141 - 00:40:47.162] Speaker 2: That's amazing.
[00:40:47.363 - 00:40:47.723] Speaker 2: No, it is.
[00:40:47.863 - 00:40:48.544] Speaker 2: It's going to be fun.
[00:40:49.285 - 00:40:50.868] Speaker 2: It's amazing.
[00:40:50.888 - 00:40:53.512] Speaker 1: In five years, we can make that a reality.
[00:40:53.952 - 00:40:56.897] Speaker 1: It's going to be incredible for the humankind.
[00:40:56.917 - 00:40:57.778] Speaker 1: It's wow.
[00:40:58.210 - 00:40:59.912] Speaker 2: Yeah, it's only happened only once.
[00:41:00.012 - 00:41:01.995] Speaker 2: Smallpox was eliminated back in 1980.
[00:41:02.336 - 00:41:06.281] Speaker 2: So, you know, polio, serious disease, it's going to be the second.
[00:41:07.623 - 00:41:07.843] Speaker 2: Wow.
[00:41:08.524 - 00:41:13.631] Speaker 1: I really truly wish that it happens faster than what we've just talked about.
[00:41:14.212 - 00:41:21.362] Speaker 1: And I hope like more than by the time the foundation gets in full fledgling the work, I believe
[00:41:21.442 - 00:41:26.007] Speaker 1: instead of not two, three, there could be multiple of them so that you lost counting.
[00:41:26.388 - 00:41:28.470] Speaker 1: That should be the goal and I'm expecting for it.
[00:41:28.890 - 00:41:30.893] Speaker 1: Well, thank you so much for doing this, sir.
[00:41:31.453 - 00:41:33.235] Speaker 1: It was a pleasure having you.
[00:41:34.296 - 00:41:42.586] Speaker 1: When I was a kid, I was growing up in school, just you were the richest person in the world.
[00:41:42.686 - 00:41:43.667] Speaker 2: Before I gave it away.
[00:41:45.229 - 00:41:46.670] Speaker 1: So your name would pop up.
[00:41:46.710 - 00:41:49.874] Speaker 1: So just to talk about you and how you made your money,
[00:41:50.146 - 00:42:10.354] Speaker 1: was a proud feeling in school and in friend circle everybody would respect you that oh you know so much about this person and that from there that kid sitting right in front of you and trying to you know being in a situation where I'm sitting with you and figuring out what's going on in your brain it's an incredible opportunity I'm so grateful
[00:42:10.466 - 00:42:11.887] Speaker 1: Thank you so much for doing this.
[00:42:12.128 - 00:42:13.289] Speaker 1: It's a dream come true.
[00:42:13.489 - 00:42:13.749] Speaker 1: Great.
[00:42:13.769 - 00:42:14.730] Speaker 1: We'll have to do it again.
[00:42:14.950 - 00:42:16.392] Speaker 1: Yes, we need to.
[00:42:16.632 - 00:42:18.333] Speaker 1: I've got you on camera now.
[00:42:18.353 - 00:42:18.854] Speaker 1: Thank you.
[00:42:19.334 - 00:42:19.775] Speaker 1: Thank you.
[00:42:19.855 - 00:42:20.275] Speaker 1: Pleasure.
[00:42:20.816 - 00:42:24.759] Speaker 1: I'm definitely a little nervous because it's a big, big, big podcast.
[00:42:24.920 - 00:42:28.483] Speaker 1: It's a big podcast, so I'm a little nervous.
[00:42:28.503 - 00:42:28.803] Speaker 1: It's time.
[00:42:28.823 - 00:42:29.164] Speaker 1: It's time.
[00:42:29.584 - 00:42:30.385] Speaker 1: But not too much.
[00:42:30.425 - 00:42:30.765] Speaker 1: Let's go.
[00:42:30.785 - 00:42:31.165] Speaker 1: I'm excited.
[00:42:33.608 - 00:42:33.668] Unknown: Hi.
[00:42:33.688 - 00:42:33.928] Speaker 1: Hi.
[00:42:34.569 - 00:42:34.829] Speaker 1: Raj.
[00:42:35.049 - 00:42:35.770] Speaker 1: How are you?
[00:42:35.790 - 00:42:36.310] Speaker 1: Very good.
[00:42:36.330 - 00:42:36.751] Speaker 1: How are you?
[00:42:37.331 - 00:42:38.312] Speaker 1: Pleasure meeting you, sir.
[00:42:38.332 - 00:42:38.713] Speaker 1: Please follow me.
[00:42:38.833 - 00:42:39.093] Speaker 1: Yes.
[00:42:39.934 - 00:42:40.274] Speaker 1: That's fine.
[00:42:40.354 - 00:42:43.317] Speaker 1: It's a pleasure to see you here sir.
[00:42:43.337 - 00:42:44.538] Speaker 1: I'm pretty nervous at this point.
[00:42:46.040 - 00:42:49.263] Speaker 1: Just bear with me a little bit.
[00:42:49.283 - 00:42:50.264] Speaker 1: Good, good.
[00:42:50.285 - 00:42:52.727] Speaker 1: I had 500,000 things to talk to you and now I'm speechless.
[00:42:53.248 - 00:42:54.449] Speaker 1: And like there's nothing in that.
[00:42:56.011 - 00:42:58.393] Speaker 1: Thank you so much for watching this episode till the end.
[00:42:58.834 - 00:43:05.401] Speaker 1: Please let us know in the comments who are the next guests that you want to see on this show.
[00:43:06.261 - 00:43:06.802] Speaker 1: Because
[00:43:07.074 - 00:43:13.131] Speaker 1: We are determined to get the best of the best minds from the world and provide you the maximum value.
[00:43:13.913 - 00:43:22.578] Speaker 1: I'll see you next time, until then keep figuring out and don't forget to share this episode with at least one person whose life will change positively.
