# ğŸ™ï¸ Listening Between The Lines: Analyzing Conversational Flow in Indian Podcasts

### *A Structured Workflow for Extracting, Cleaning, Translating & Analyzing YouTube Podcast Transcripts*

## ğŸ“Œ **Project Overview**

This project implements a complete pipeline for automatically collecting, cleaning, translating, and performing basic analysis on podcast transcripts from YouTube.
It is designed as a modular, layered architecture that processes raw unstructured content into structured text ready for downstream research.

The system supports:

* Automated transcript extraction from YouTube
* Translation of non-English transcripts
* Cleaning and normalization
* Sentence segmentation
* Basic text analytics
* Batch processing for multiple episodes

This repository contains all scripts, data folders, and sample outputs needed to reproduce the workflow.

## ğŸ—‚ï¸ **Repository Structure**

```
podcast_analysis_project/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_transcripts/        # Original transcripts (JSON or text)
â”‚   â”œâ”€â”€ cleaned_transcripts/    # Cleaned/translated text files
â”‚   â””â”€â”€ metadata/               # Episode metadata (optional)
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ tables/                  # Word/sentence counts
â”‚   â””â”€â”€ samples/                # Extracted question lists
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ batch_download.py       # Batch transcript downloader
â”‚   â”œâ”€â”€ convert_json_to_text.py # JSON â†’ TXT converter
â”‚   â”œâ”€â”€ batch_translate_to_english.pyâ”‚
â”‚   â”œâ”€â”€ extract_questions.py
â”‚   â”œâ”€â”€ text_stats.py        # Word & sentence count generator
â”‚   â””â”€â”€ episode_list.txt        # List of YouTube URLs
â”‚
â”‚
â””â”€â”€ README.md                   # Project documentation
```

## ğŸ§± **System Architecture**

The project is built using a 5-layer pipeline:

1. **Input Layer**

   * Stores YouTube URLs for all episodes
   * Provides controlled input for batch processing

2. **Transcript Extraction Layer**

   * Uses `youtube_transcript_api` to download transcripts
   * Saves raw JSON files with time-stamped segments

3. **Translation & Cleaning Layer**

   * Translates Hindi / Hinglish episodes to English
   * Cleans artifacts, spacing, and structural inconsistencies

4. **Preprocessing Layer**

   * Sentence segmentation (NLTK)
   * Tokenization
   * Text normalization
   * Ready for further NLP tasks

5. **Output & Analysis Layer**

   * Extracts user questions
   * Computes word & sentence statistics
   * Produces structured output in `/outputs/`


## âš™ï¸ **Installation**

### 1ï¸âƒ£ Clone the repository

```bash
git clone https://github.com/Dhanyaravikumarsuchithra/Capstone-Listening-Between-the-Lines.git
cd podcast_analysis_project
```

### 2ï¸âƒ£ Install dependencies

You should create a virtual environment (recommended):

```bash
pip install youtube-transcript-api deep-translator nltk pandas
```

### 3ï¸âƒ£ Download NLTK data

```python
import nltk
nltk.download('punkt')
```

---

## ğŸš€ **How to Run the Pipeline**

### **Step 1 â€” Add YouTube URLs**

Edit:

```
src/episode_list.txt
```

One URL per line.

---

### **Step 2 â€” Download Transcripts**

```bash
python src/batch_download.py
```

Raw transcripts will be saved inside:

```
data/raw_transcripts/
```

---

### **Step 3 â€” Translate (only for Hindi episodes)**

```bash
python src/batch_translate_to_english.py
```

---

### **Step 4 â€” Clean Transcripts**

```bash
python src/clean_transcript.py
```

---

### **Step 5 â€” Extract Questions**

```bash
python src/extract_questions.py
```

---

### **Step 6 â€” Compute Word & Sentence Stats**

```bash
python src/text_stats.py
```

Outputs stored in:

```
outputs/tables/stats
```

---

## ğŸ“Š Example Output

### **Word & Sentence Statistics**

```
episode,word_count,sentence_count
ep001_cleaned.txt,6698,312
ep002_cleaned.txt,6624,298
ep003_cleaned.txt,11430,629
...
```

### **Extracted Questions (sample)**

```
What is the most misunderstood thing about you?
Do you think AI poses a global threat?
Why do you believe India is becoming a global talent hub?
...
```

---

## ğŸ§ª **Technologies Used**

| Component             | Library / Tool           |
| --------------------- | ------------------------ |
| Transcript download   | youtube-transcript-api   |
| Translation           | deep-translator          |
| Cleaning              | Python string processing |
| Sentence segmentation | NLTK                     |
| Statistics            | pandas                   |
| Data storage          | Local filesystem         |

---

## ğŸ§¾ **Project Goals**

This repository aims to:

* Build a structured and reproducible workflow
* Enable analysis of conversational patterns in podcasts
* Provide early exploratory results for further NLP models
* Form the basis for next-semester research

---

## ğŸ™Œ **Team Members**

* Bhavini Sai Mallu
* Sameeksha Rao

